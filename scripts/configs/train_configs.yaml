# This file contains default training parameters assuming access to A100-80GBs
allenai/tulu-2-7b:
  model: 'allenai/tulu-2-7b'
  tokenizer: 'allenai/tulu-2-7b'
  chat_template: 'tulu'
  num_gpus: 4
  total_batch_size: ...
  batch_size_per_gpu: ...
  batch_size: ...
  max_seq_len: ...
  use_flash_attn: True
  bf16: True
  batch_size_per_gpu: ...
meta-llama/Llama-2-7b-chat-hf:
  model: 'meta-llama/Llama-2-7b-chat-hf'
  tokenizer: 'meta-llama/Llama-2-7b-chat-hf'
  chat_template: 'llama-2'
  num_gpus: 4
  total_batch_size: ...
  batch_size_per_gpu: ...
  batch_size: ...
  max_seq_len: ...
  use_flash_attn: True
  bf16: True
  batch_size_per_gpu: ...
TinyLlama/TinyLlama-1.1B-Chat-v1.0:
  model: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
  tokenizer: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
  chat_template: 'llama-2'
  num_gpus: 1
  total_batch_size: ...
  batch_size_per_gpu: ...
  batch_size: ...
  max_seq_len: ...
  use_flash_attn: True
  bf16: True
  batch_size_per_gpu: ...